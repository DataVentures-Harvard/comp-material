{
 "metadata": {
  "name": "",
  "signature": "sha256:4b1579d0df38a48d82c2b90ab2051efc2550a5e73db8a11fd2b3c5700ff9d874"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Basic Machine Learning I: Clustering and Dimensionality Reduction</font></font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"#5C3317\">Finding structure in the unstructured</font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Ventures, Harvard College Division | Spring 2015 | Session 4A: K-Means Clustering and Principal Components Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Today we will start talk about \"unsupervised learning\" with the topic clustering. Machine learning also encompasses supervised learning such as the regressions we first covered at the start of the comp. In addition to clustering, we discuss dimensionality reduction, which is a particularly important tool in the age of big data. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img style=\"width:400px\" src=\"http://robotonomics.files.wordpress.com/2014/02/behavioral-economics-and-big-data.png\" alt=\"Optimize\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Clustering</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clustering is one way to find structure in unstructured data, and a popular and fairly simple algorithm is $k$-means clustering. Let us suppose we have an observation **x**. In practice **x** is a vector where each componen $x_i$ can correspond to some characteristic, and we can plot **x** in $n$-dimensional space. It is fairly easy to visualize the 2D case: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img style=\"width:350px\" src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/KMeans-Gaussian-data.svg/2000px-KMeans-Gaussian-data.svg.png\" alt=\"K-Means\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the $k$-means algorithm, we (**1**) first randomly pick our $k$ centroids, and then we assign each of our data points to one of these clusters. We (**2**) then find the average or mean coordinate for each of the $k$ clusters and then re-assign all the points based on which of the $k$ means they are closest to. We (**3**) continue until we no longer have made any reassignments. This iterative algorithm is actually simple enough to code up that we will code up a $3$-means algorithm corresponding to the above 2D example. Note that the centroid is not necessarily always a data points itself, but rather a coordinate in the $n$-d space in which the data exists. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# take the first k points to be the k centroids \n",
      "def initialize(lst, k):\n",
      "    return lst[:k]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# return the average centroid of a cluster group\n",
      "def avg(lst):\n",
      "    sum_x, sum_y = 0, 0\n",
      "    for i in range(len(lst)):\n",
      "        sum_x += lst[i][0]\n",
      "        sum_y += lst[i][1]\n",
      "    avg_x = sum_x / float(len(lst))\n",
      "    avg_y = sum_y / float(len(lst))     \n",
      "    return (avg_x, avg_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# return the euclidean distance between two points in 2D\n",
      "import math\n",
      "def distance(p1, p2):\n",
      "    return math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# return index of the closest centroid (cluster)\n",
      "def min_dist(pt, points):\n",
      "    min_distance = distance(pt, points[0])\n",
      "    index = 0\n",
      "    iteration = 0\n",
      "    for point in points:\n",
      "        if min_distance > distance(pt, point): \n",
      "            min_distance = distance(pt, point)\n",
      "            index = iteration\n",
      "        iteration += 1\n",
      "    return index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# keep computing new centroids and reassign clusters until convergence\n",
      "def k_means(lst, centroids):\n",
      "    clusters = [[centroid] for centroid in centroids]\n",
      "    classifications = list(np.ones(len(lst)))\n",
      "    for i in range(len(lst)):\n",
      "        cluster_assignment = min_dist(lst[i], centroids)\n",
      "        classifications[i] = cluster_assignment\n",
      "        clusters[cluster_assignment].append(lst[i])\n",
      "    newCentroids = []\n",
      "    for cluster in clusters:\n",
      "        newCentroids.append(avg(cluster[1:]))\n",
      "    if set(centroids)==set(newCentroids): return classifications\n",
      "    else: return k_means(lst, newCentroids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# main clustering algorithm\n",
      "def clustering(lst, k):\n",
      "    centroids = initialize(lst, k)\n",
      "    return k_means(lst, centroids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# example \n",
      "print clustering([(1,1),(2,2),(33,33),(44,44),(660,661),(666,666)],2)\n",
      "print clustering([(1,1),(2,2),(33,33),(44,44),(660,661),(666,666)],3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0, 0, 0, 1, 1]\n",
        "[0, 0, 1, 1, 2, 2]\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above toy example, we've found the clusters or groupings that we would intuitively expect to see in our simple 2D case. Of course, as always, Python writers have come up with a library and function for what we want to do (although it's really fun to develop things from scratch to solidify how the concept works in your head). Note that while we talk about the more popular clustering algorithm of $k$-means, there are many more interesting algos described in <a href = \"http://scikit-learn.org/stable/modules/clustering.html\">Scikit-Learn</a>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cluster\n",
      "two_means = cluster.KMeans(n_clusters=2)\n",
      "print two_means.fit_predict([(1,1),(2,2),(33,33),(44,44),(660,661),(666,666)])\n",
      "three_means = cluster.KMeans(n_clusters=3)\n",
      "print three_means.fit_predict([(1,1),(2,2),(33,33),(44,44),(660,661),(666,666)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 0 0 0 1 1]\n",
        "[2 2 0 0 1 1]\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Dimensionality Reduction</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If each data point can be described by 1000 different characteristics, this would be really difficult to visualize, and probably difficult to computationally process. In practice, a lot of characteristics are correlated with one another, e.g. if we were looking at economic data, the Gross National Product $GNP$ would likely be positively correlated with the Gross Domestic Product $GDP$ since while they are not entirely the same metric they are extremely similar. In the 2D case, we might wish to collapse points onto a line (why is this **not** similar to regression?). Similarly, in the 3D case, we could imagine collapsing onto a plane or onto a line as well. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img style=\"width:700px\" src=\"http://www.nlpca.org/fig_pca_principal_component_analysis.png\" alt=\"PCA\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We apply dimensionality reduction by principal components analysis. This algorithm relies on eigenvalues and singular value decomposition, linear algebra beyond the scope of the comp, but as crude intuition think of the matrix formation as an expression of multiple dimensions - we can remove a few dimensions and we reduce the size of the matrix we work with, and in doing so, we collapse elements into a new coordinate system (see above diagram). Note that unless all the points originally happened to already lie on the line, plane, or hyperplane we collapse to, your projections yield approximations of the original information. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While with $k$-means clustering, there is really no \"gold-standard\" way of selecting the number of clusters, the way in which we decide to select the number of principal components $k < n$, where $n$ is the original dimension of the data, is that we select the smallest possible $k$ such that 99% of the original variance of the data is retained (in which case do we retain 100% of the original variance?). We use `sklearn` as before to apply <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">PCA</a>:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np \n",
      "from sklearn import decomposition\n",
      "data = [(1,1),(2,2),(33,33),(44,44),(660,661),(666,666)]\n",
      "pca_all = decomposition.PCA(n_components=2)\n",
      "pca_all.fit(data)\n",
      "decomposition.PCA(copy=True, n_components=2, whiten=False)\n",
      "\n",
      "pca_linear = decomposition.PCA(n_components=1)\n",
      "pca_linear.fit(data)\n",
      "decomposition.PCA(copy=True, n_components=1, whiten=False)\n",
      "\n",
      "print \"If we keep all components:\",(pca_all.explained_variance_ratio_)\n",
      "print \"If we use only 1 component:\",(pca_linear.explained_variance_ratio_) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " If we keep all components: [  9.99999772e-01   2.28297377e-07]\n",
        "If we use only 1 component: [ 0.99999977]\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Does a straight-line collapse of the 2D data work well in this contrived example? Why (take a look at the actual toy dataset)? "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Mini-Project 3B: \"Predict\" Blocking Groups</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's say we have a bunch of data on freshmen: current dorm, prospective concentration, prospective secondary, current student organization affiliations. Imaginably, freshmen will tend to block with students they already live close to and have come to know or with students with shared intellectual and/or extracurricular interests. Whereas for previous projects we've had nice clean datasets to work with, for this project we will actually be collecting data for this mini-project by collecting data from fellow compers. Of course, you guys are of different years but it would be fun to have imagined if everyone here starting off as classmates together in the same year. Let's see where \"blockships\" could have come from \"friendships\"! Take this to be a fun bonding assignment and to meet other compers. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Advanced Project 4B: Digit Recognition (\"Scify\" Style $-$ Well, Almost)</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://www.kaggle.com/c/digit-recognizer\">Digit Recognizer</a> is still on ongoing Kaggle competition from 2012. If you and your teammmates are up for a pure challenge, note that $k$-means clustering can be used to analyze parts of this dataset. While we won't have time to guide you with this one, have fun with it and if you think you and your team have made sizable progress, go ahead and submit $-$ there's nothing to lose! "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<center><font face='Garamond'>\u00a9 Data Ventures Global</font></center>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}