{
 "metadata": {
  "name": "",
  "signature": "sha256:4cdacf2df09fbc1e1f08d975b3c3e128d25ae2a3874906325880e0ea5b1264e0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Econometrics II: Time Series and Survival Analysis</font></font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"#5C3317\">i.e. And forecasting, or how to code a crystal ball</font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Ventures, Harvard College Division | Spring 2015 | Session 2A: Temporal Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Welcome back to another lecture session of the comp! We hope you learned quite a bit with the regressions project in the last hands-on session. Today, we'll focus heavily on the concept of time, since if you've watched Interstellar or Inception, you'll probably definitely appreciate that everything we're interested essentially comes down to this core concept, especially data. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://jameslaurencedyer.files.wordpress.com/2014/06/time-travel.jpg\" alt=\"Time\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Time series is a powerful field in statistics applied widely in operations research, engineering, weather forecasting, and finance. The basic tenet is fairly intuitive: tomorrow depends a lot on today. Any time series analysis at the most basic level has to capture resolution, i.e. time increments, and horizon, i.e. how far out of a timeframe we are considering. The farther out we try to model, the more uncertainty our computations must capture. Long horizons with high resolution are generally limited by the limited past data, often requiring aggregating various low resolution time series data. As an extreme example if we want to model 100-years out on minute-by-minute increments, we likely won't find data of several decades on minute increments. It is more likely that we will find data starting and ending on week by week or so, and we would consolidate this data to produce our forecasts. For simplicity, we assume that our dependent variable is always nonnegative (although our changes therein may not be). \n",
      "\n",
      "Survival analysis is another important temporal model that measures time lapses until the occurrence of an event, e.g. time until cell death or electrical failure depending on system of interest. In this session, we starting exploring both subfields. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Time Series</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Time series are used characteristically in financial time series. If you're interested in finance, technical analysis (as opposed to fundamental analysis or value investing) relies on tools like time series to forecast and (profitably) trade on the markets. You might have already seen some sort of a time series in Econ 10 or in Yahoo's weather charts. In fact, statistical indication for global warming can be given by long-standing temperature trends after account seasonal and random fluctuation in weather. Can you think of other important scenarios in which modelling time points in fairly important in your (propespective) field of study? \n",
      "\n",
      "In the following primer, we cover the basic concepts in the field, but note that many more advanced topics like Kalman filters, are excluded. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "<font color=\"Teal\">PROPERTIES</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Components**. Key underlying components of a time series are (**1**) trend, (**2**) seasonality, and (**3**) noise. Trend refers to how the mean of the quantity of interest changes over time. Seasonality refers to business cycles or other known calendar-related changes. Noise refers to the typical random fluctuation of a time series around the trend and cyclical patterns. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Stationarity**. The idea of stationarity is assumed in certain computations over time series and essentially refers to the mean, variance, and autocorrelation not changing through time. *Strong stationarity* refers to the fact that for a time series indexed from time $t \\geq 0$, $X_t$, we have $(X_t, X_{t+k}) \\sim (X_j, X_{j+k})$, i.e. equal time *increments* result in identically distributed time series. In weak stationarity, the mean and $k$-lag autocorrelation is independent of time, i.e. $E(X_t) = \\mu$ and $Cov(X_t, X_{t+k}) = \\gamma_k$. Autocorrelation is a key metric which we will describe below, but for now as a very rudimentary recap (re: `Session 0B`) how does a correlation relate to covariance?  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case the variance is not constant, and thus the autocorrelations are not independent of time, transforming the data such as by a logarithmic function can produce usually stationarity. In the case of a trend, we can fit the data to any sort of linear or polynomial model and can typically produce residuals with stationarity. Another common technique, and one used in a time series model we discuss below, is to \"difference\" the data: $Y_t = X_t - X_{t-1}$, where a $k$-order difference is $Y_{t,k} = X_t - X_{t-1} - (X_{t-1} - X_{t-2}) - ... - (X_{t-k+1} - X_{t-k})$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "<font color=\"Teal\">METRICS</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Moving Average.** As mentioned, a time series can be broken down into the (**1**) trend (**2**) cyclical changes (**3**) random, short-term variations. We can apply a linear or polynomial model in understanding the long-term trend. We can also using the rather simple notion of a \"moving average\" such as a rolling mean (simple moving average), where the entire time interval of interest is broken into smaller *overlapping* timeframes, and the average for each time frame is computed. Of course, we just mentioned earlier that in a time series the basic tenet is that tomorrow depends a lot on today -- points from a century would not be nearly as relevant. Thus in the concept of an exponential moving averaging, more weight is given to the more recent time point when computing averages. We implement these functions below, but what libraries in python can you use to accomplish the same thing? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rolling mean (smooth moving average)\n",
      "import numpy as np\n",
      "def rolling_mean(time_series, interval): \n",
      "    return np.convolve(time_series, np.ones(int(interval))/float(interval), 'valid')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are packing in a number of things with the above short code. What does `numpy`'s `convolve` method do? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# exponentially weighted moving average\n",
      "def ewma(time_series, interval):\n",
      "    length = len(time_series)\n",
      "    increments = np.linspace(-1., 0., interval)\n",
      "    exponentials = np.exp(increments)\n",
      "    weights = float(exponentials) / exponentials.sum()\n",
      "    expoMovAvg = np.convolve(time_series, weights)[:length]\n",
      "    expoMovAvg[:interval] = expoMovAvg[interval]\n",
      "    return expoMovAvg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now in this next function, what does `linspace` do here? Alternatively, note that if we were working with a `pandas` dataframe, we could call this module's own `pandas.rolling_mean(dataframe, window)` and `pandas.ewma(dataframe, span=window)`, respectively. Whatever solution you use, make sure you are using it correctly (test your functions or double-check the source documentation)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Autocorrelation.** Autocorrelation, sometimes referred to as serial correlation, builds upon the exponential moving average's intuitive concept that closer time points are correlated with surrounding time points. If we can find some sort of a long-term trend, we would expect to find autocorrelation in the data. A key parameter is the autocorrelation calculation is the *lag* which is the time frame by which the entire series is shifted. Can you see why that when `lag=0` the autocorrelation is `1` (re: `Session 0B`)? Intuitively, when the lag goes to infinity, what would we expect the autocorrelation to go to? We can probably implement the autocorrelation function within a few lines, but let's introduce the `statsmodel` module in python which contains the package `tsa` for time series analysis and which we will use later for model fitting as well. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import statsmodels\n",
      "def acf(time_series, lags): \n",
      "    return statsmodels.tsa.stattools.acf(time_series, nlags=lags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "<font color=\"Teal\">MODELS</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Autoregressive (AR) Model**. We have the following autoregressive process of order $p$ with time series $X_t$, model parameters $\\beta_i$, and random noise $\\epsilon_t$ (the technical term for this is \"white noise\" in signal processing). \n",
      "This form looks sort of familiar, doesn't it? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \\; AR(p) \\; : \\; \\; X_t \\; = \\; \\Sigma_{i = 1}^{p} \\; \\beta_i \\; X_{t-i} \\; + \\; \\epsilon_t $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Moving Average (MA) Model**. We have the following autoregressive process of order $p$ with time series $X_t$, model parameters $\\beta_i$, and random noise $\\epsilon_t$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\; MA(p) \\; : \\; \\; X_t \\; = \\; \\Sigma_{i = 1}^{p} \\; \\beta_i \\; \\epsilon_{t-i} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Autoregressive Moving Average (ARMA) Model**. The above two ideas can be combined into the following ARMA model of orders $p$, $q$ with model parameters $\\beta_i$ and $\\gamma_i$: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \\; ARMA(p, q) \\; : \\; \\; X_t \\; = \\; \\Sigma_{i = 1}^{p} \\; \\beta_i \\; X_{t-i} \\; + \\; \\Sigma_{i = 1}^{q} \\; \\gamma_i \\; \\epsilon_{t-i}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Autoregressive Integrated Moving Average (ARIMA) Model**. We had said before that many time series models assume some sort of stationarity and in the event that a time series $X_t$ violates that assumption, we might be able to look instead at the first-order difference $Y_t = X_t - X_{t-1}$, or the second-order difference $Y_t = X_t - 2X_{t-1} + X_{t-2}$. $X_t$ is an $ARIMA(p, q, d)$ process if the $d$-order difference $Y_t$ is an $ARMA(p, q)$ process. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've covered the most major models used to describe time series. Let's again turn to the Python module `statsmodels` for further functions for time series analysis within `tsa`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from statsmodels.tsa import arima_model\n",
      "def arma_parameters(time_series, p, q):\n",
      "    model = arima_model.ARMA(time_series, order = (p, q)).fit()\n",
      "    print \"Parameters\" \n",
      "    print model.params\n",
      "    print \"Residuals\" \n",
      "    print model.resid\n",
      "    return "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visually, we can use what is known as a \"qqplot\", which extends to beyond just time series, to determine how well our model worked, where a good fit should yield residuals that lie along the $y=x$ line, such as in the example plot below:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://cartesianfaith.files.wordpress.com/2012/02/xom-qq-plot.png\" alt=\"Time\" style=\"width:300px\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from statsmodels.graphics.api import qqplot\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def residuals(time_series, p, q):\n",
      "    model = arima_model.ARMA(time_series, order = (p, q)).fit()\n",
      "    return model.resid \n",
      "\n",
      "def qqplot(resids):\n",
      "    fig = plt.figure(figsize=(12,8))\n",
      "    ax = fig.add_subplot(111)\n",
      "    fig = qqplot(resids, line='q', ax=ax, fit=True)\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even if you implement your own prototype, it's a good idea to check your results against know working functions and to be able to read the <a href=\"http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.arima_model.ARMA.html\">specs</a>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Survival Analysis</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Survival analysis is often used in engineering and the applied sciences, especially pharmaceutical studies. The core concepts are the survival function $S(t)$, the hazard function $h(t)$, and the cumulative hazard function $\\Lambda(t)$. Based on the basics from `Session 0B`, how else can we reformulate the equations below (think complements and densities)?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ S(t) = P(T > t) $$\n",
      "\n",
      "$$ h(t) = - S'(t) \\; / \\; S(t) $$\n",
      "\n",
      "$$ \\Lambda(t) = - log \\; S(t)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$S(t)$ captures the probability that the event of interest (e.g. biological death, device failure, drug filtration) takes more than time $t$ to occur, $h(t)$ provides a sense of the event rate given survival at least up to time $t$, and $\\Lambda(t)$ indicates the \"accumulated\" hazard given by $h(t)$ at time $t$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Kaplan-Meier**. Survival analysis, much like time series, is a vast field that deserves an entire semester-long course to itself. We thus used the mostly used estimator here, known as Kaplan-Meier. This is a fairly easy estimator to code up and while there are Python libraries for this, we will write a function to capture the formula:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \\hat S (t) = \\Pi_{t_i < t} \\; (n_i \\; - \\; d_i) \\; / \\; n_i $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function for list with surviving numbers at each time interval \n",
      "def kaplan_meier(lst):\n",
      "    km_estimates = [1]\n",
      "    time_intervals = len(lst)\n",
      "    initial_population = lst[0]\n",
      "    deaths = 0\n",
      "    previous_count = initial_population\n",
      "    current_product = 1\n",
      "    for i in range(time_intervals-1):\n",
      "        death = previous_count - lst[i+1]\n",
      "        print previous_count, lst[i+1]\n",
      "        current_product *= (previous_count - death) / float(previous_count) \n",
      "        km_estimates.append(current_product)\n",
      "        previous_count = lst[i+1]\n",
      "    return km_estimates "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Mini Project 2B: Global Copper Markets</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's consider the 1951-1975 global copper <a href=\"http://statsmodels.sourceforge.net/stable/datasets/generated/copper.html\">markets</a>, information on which comes as one of the toy datasets in the `statsmodels` module. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import statsmodels.api as statsm\n",
      "print statsm.datasets.copper.NOTE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of Observations - 25\n",
        "\n",
        "Number of Variables - 6\n",
        "\n",
        "Variable name definitions::\n",
        "\n",
        "    WORLDCONSUMPTION - World consumption of copper (in 1000 metric tons)\n",
        "    COPPERPRICE - Constant dollar adjusted price of copper\n",
        "    INCOMEINDEX - An index of real per capita income (base 1970)\n",
        "    ALUMPRICE - The price of aluminum\n",
        "    INVENTORYINDEX - A measure of annual manufacturer inventory trend\n",
        "    TIME - A time trend\n",
        "\n",
        "Years are included in the data file though not returned by load.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can fine the complete documentation at: http://statsmodels.sourceforge.net/devel/tsa.html."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Advanced Project 2B: Tourism Forecasts</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ready for a(n optional) challenge? Travelling is fun and tourism is excellent for various local economies. We will focus on a past 2010 \n",
      "<a href=\"http://www.kaggle.com/c/tourism1\">Tourism Forecasting</a> Kaggle competition. Again, ask yourselves the same key questions as in the last mini-project: what methods are appropriate and what computational tools are available? While we won't have enough time to walk you through this one, it can be worth looking at how to head beyond the basic ideas to make sense of this dataset. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<center><font face='Garamond'>\u00a9 Data Ventures Global</font></center>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}