{
 "metadata": {
  "name": "",
  "signature": "sha256:72a1ea4e67caa33bb7a82ea8800f81a200f80ed122081f8f34c339dd241b5aa9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Basic Machine Learning II: Support Vector Machines, Decision Trees</font></font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"#5C3317\">The penultimate computational finale</font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Ventures, Harvard College Division | Spring 2015 | Session 3A: Linear, Integer (Discrete), and Nonlinear (namely, Convex) Optimization Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alas, we're nearing the end (where did the semester go!). We will talk about a few more major techniques in machine learning. We head back to supervised learning with the concept of \"support vector machines\" and as well as the subject of \"decision trees.\" "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img src=\"http://d3sdoylwcs36el.cloudfront.net/google-panda-farmer-update-machine-learning_000010874966XSmall.jpg\" alt=\"ML\" style=\"width:300px\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Support Vector Machines</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support vector machines (SVMs) are an additional we can add to what we've seen before. When we had first mentioned decision boundaries between two surfaces, our goal was to find a boundary that separated two groups. Support vector machines ensures that the boundary is optimal in that the separation between the group has maximum margin (distance). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img src=\"http://docs.opencv.org/_images/optimal-hyperplane.png\" alt=\"ML\" style=\"width:200px\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us assume we have a linear decision boundary as above, then in Python we could apply an <a href=\"http://scikit-learn.org/stable/modules/svm.html\">SVM</a> in the following toy example where our $x_1$ coordinate corresponds to the proportion of tall cell features and the $x_2$ corresponds to the proportion of extrathyroidal extensions to determine the aggressiveness of a certain form of cancer, with $0$ being non-aggressive and $1$ being aggressive. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "X = [[0.9, 0.7], [0.7, 0.2], [0.8, 0.8], [0.1, 0.2], [0.3, 0.9], [0.6, 0.9]]\n",
      "y = [1, 0, 1, 0, 1, 1]\n",
      "clf = svm.SVC()\n",
      "clf.fit(X, y)  \n",
      "clf.predict([[0.2, 0.9]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([1])"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Decision Trees</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With decision trees, we successively split groups based on the most deciding variable. Let us consider for example the decision to study abroad or not. Relevant factors include: (a) availability of funding, (b) expensiveness of host country/city, (c) cultural richness of host town, (d) intellectual rigor of host institution. Of these four factors, the one that most determines your decision to study abroad is (a), where funding above $4000 tends to lead to actual travel and otherwise not. Within the first subgroup (over 4000 dollars of funding), the next important deciding factor is (d) intellectual rigor of planned study, and in the second subgroup (less than 4000 dollars of funding), the next important deciding factor is (b) the costliness of the destination. Pretty intuitive so far, right? But how we quantify the most important deciding factor? There are a few metrics decision tree classifers on which to decide how split a group and further individually split each subgroup based on the rest of the variables. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Information gain**. $- \\; \\Sigma_{i=1}^{m} \\; \\; f_i \\; log_2 \\; (f_i)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Gini impurity**. $1 \\; - \\; \\Sigma_{i = 1}^{m} \\; f_i$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a subgroup, we consider a set of variables $1, ..., m$. If the information gain is 0, we have perfect purity, and if 1 we have no purity. For Gini, the same follows respectively with values of 0 and 0.5. The closer we are to perfect purity for some variable $i$, the better that variable decides a particular (sub)group. The Gini impurity is the default metric for `sklearn`'s decision tree <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">classifier</a>. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import tree\n",
      "factors = [[4500, 7, 10, 10], [6000, 10, 5, 10], [5500, 5, 4, 10], \n",
      "           [3000, 10, 7, 7], [1000, 12, 10, 10]]\n",
      "study_abroad = [1, 1, 1, 0, 0]\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(factors, study_abroad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.predict([6100,7,5,10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "array([1])"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above, the list corresponds to funding, 10-scale destination costliness, 10-scale cultural richness, and 10-scale intellectual rigor. With generous funding, the decision tree reasonably predicts a student's choice to study abroad (indicator of $1$). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Mini-Project 5B: Standard Wild Card</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the last `B` session, we've described the sort of data you could collect and you've found quite a bit of flexibility with sort of data you were working with. In this session, we go one step further, and let this part be entirely up to you, where your task is to find a practical or whimsical dataset with a possible decision tree structure, and find the underlying nodes. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Advanced Project 5B: The Galaxy Challenge</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For one last optional challenge, try out decision trees with <a href=\"https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/details/the-galaxy-zoo-decision-tree\">Galaxy Zoo</a>, an old 2014 Kaggle competition.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<center><font face='Garamond'>\u00a9 Data Ventures Global</font></center>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}