{
 "metadata": {
  "name": "",
  "signature": "sha256:dc4aa307b978269c1cb4839cd2ba145e1268738bfd5d5ff37e4c19ec81cfbc5b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Operations Research: Linear and Nonlinear Optimization</font></font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"#5C3317\">i.e. Decision-making at its rudiments</font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Ventures, Harvard College Division | Spring 2015 | Session 3A: Linear, Integer (Discrete), and Nonlinear (namely, Convex) Optimization Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Welcome back! This week will fairly short but we talk about a field that people don't most often think of when consider data. Operations research, or decision science, is a huge umbrella term and includes the previous body of statistical methods that we've discussed as well as optimization techniques. Unlike previous analytical solutions, some optimization problems are known \"NP-hard\", which means that they can be computationally expensive. Many optimization algorithms are iterative and while we will discuss the geometry behind their methods, we will make use of various solvers which will be adequate for most practical applications. What is key behind any of these approaches is a proper formulation of the optimization problem at hand. In this session, we will cover three majorcategories of optimization problems: linear, discrete (integer), and nonlinear by way of convex optimization. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img style=\"width:250px\" src=\"http://blog.sitedart.net/Portals/147314/images/keep-calm-and-start-optimizing.png\" alt=\"Optimize\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Linear Optimization</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear optimization is relatively the more straightforward problem, where our optimization object is a linear function (a polynomial of degree 1), as are all of its constraints. We have the following prototype for a linear optimization problem where we have vectors $c, b$ and matrix $A$ associated with the data given, and decision variables $x$ constrained to be positive. By negating appropriate we can convert any linear optimization formula to the standard form below. A maximization problem can also be converted to a minimization problem , as maximizing $f(x)$ is the same as the negated value of minimizing $-f(x)$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center> minimize $f(x) = c^T x$ </center>\n",
      "\n",
      "<center> subject to $A(x) \\geq b$ </center>\n",
      "\n",
      "<center> $x \\geq 0$ </center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case of a unique optimal solution, the constraints form a convex polyhedron (any convex set is such that any line joining two points in the set is also part of the set). The optimal solution exists among one of vertices of this convex polyhedron. \n",
      "\n",
      "<center>\n",
      "<img style=\"width:250px\" src=\"http://www.softmath.com/tutorials-3/cramer\u2019s-rule/articles_imgs/4320/exact_57.jpg\" alt=\"Convex Polyhedron\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Discrete Optimization</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Many discrete optimization problems can be formulated as linear optimization problems with simply the additional constraint that the decision variables be integral, e.g. $x \\in \\{0, 1\\}$ in a binary decision problem. Discrete and continuous decision variables can be combined into what is known as a mixed integer linear problem (MILP). The way discrete optimization problems are solved in practice are by methods such as \"branch and bound\" where we first solve the linear optimization version of the problem (which relaxes the integrality constraints) and then impose additional constraints satisfied by the discrete optimization problem but not the linear relaxation of the discrete problem (each constraint creates a \"cut\"). We do this iteratively until we arrive at an integral solution to some sub-problem of linear relaxation. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Convex Optimization</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convex optimization actually generalizes upon the above, as a linear function is convex (it's also concave). This comes from the definition of convexity which states that for some convex function $f: \\; \\; f(\\lambda x_1 + (1 - \\lambda) x_2) \\leq \\lambda f (x_1) + (1 - \\lambda) f (x_2) \\; \\; \\forall \\; \\; \\lambda \\in [0,1]$.\n",
      "\n",
      "Additional conditions on convexity use calculus and in the multidimensional case require more advanced linear algebra-based conditions over what are known as Hessian matrices. For now, the geometric intuition provided by the above definition will suffice (think tangent line and secant for the 1D case). While we won't be implementing the algorithms for convex optimization from scratch, know that convex optimization problems can be solved by iterative, numerical methods such as gradient descent (we discussed this briefly in `Session 1A`. Convex objective functions have a single local optimum which is also the global optimum, and so a solution can be found. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img style=\"width:350px\" src=\"http://www.dt.e-technik.uni-dortmund.de/Seitenbilder/convex.jpeg\" alt=\"Convex Optimization\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can formulate the general convex optimization problem as follows, for convex functions $f, g_i$ and linear functions $h$:\n",
      "\n",
      "$$ minimize \\; \\; \\; f(x) $$\n",
      "\n",
      "$$ s. \\; t. \\; g_i(x) \\leq 0 \\; \\; \\forall \\; i \\in \\{1, ..., m\\}$$ \n",
      "\n",
      "$$ \\; \\; \\; \\; h_i(x) = 0 \\; \\; \\forall \\; i \\in \\{1, ..., n\\}$$ "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Mini Project 3B: Formulating the last semester</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This week's A and B sessions are short enough to combine into a single week. How would we formulate the problem of being a senior, where our decision variables are number of \"hard\" classes to take, number of \"moderately difficult\" classes, and number of \"light\" classes to take, where our constraints are sleep and we seek to optimize learning. That is, hard classes take 15 hours each, moderately difficult classes take 9 hours a week, and light classes take 4 hours a week, and respectively these classes also give 7, 3, 1 units of stress. Hard classes give 5 units of utility, moderately difficult classes give 3 units of utility, and light classes give only a single unit of utility. Because you're a senior and have to write a thesis / apply for a job / apply for grad school / save the world, you cannot spend more than 52 hours a week on classes and cannot stress more than 21 \"stress units\", but you wish to maximize your intellectual utility (outside the context of working on your thesis). Well let us define the numbers of hard, moderately difficult, and lights courses as $x_1, x_2, x_3$. Then we have the following optimization problem: \n",
      "\n",
      "$$ maximize \\; \\; 5 x_1 + 3 x_2 + x_3 $$\n",
      "\n",
      "$$ s. \\; t. \\; \\; 15 x_1 + 9 x_2 + 3 x_3 \\leq 52 $$\n",
      "\n",
      "$$  \\; \\; \\; \\; \\; \\; 7 x_1 + 3 x_2 + 1 x_3 \\leq 21 $$\n",
      "\n",
      "\n",
      "$$ x_1, x_2, x_3 \\geq 0 $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From our discussion, we can use Python's `cvxopt` software package for linear optimization problems, defining the matrices $A, b, c$ accordingly. For there to be a unique optimal solution, what property must $A$ necessarily have (think rank)? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = matrix([[15.0, 7.0, -1.0, 0.0, 0.0], [9.0, 3.0, 0.0, -1.0, 0.0], [3.0, 1.0, 0.0, 0.0, -1.0]])\n",
      "b = matrix([52.0, 21.0, 0.0, 0.0, 0.0])\n",
      "c = matrix([-5.0, -3.0, -1.0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cvxopt import matrix, solvers\n",
      "def linear_solver(A, b, c):\n",
      "    solution = solvers.lp(c,A,b)\n",
      "    return solution['x']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print linear_solver(A, b, c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "     pcost       dcost       gap    pres   dres   k/t\n",
        " 0: -1.7204e+01 -9.4673e+01  1e+01  6e-02  4e+00  1e+00\n",
        " 1: -1.7202e+01 -2.4672e+01  6e-01  6e-03  4e-01  3e-01\n",
        " 2: -1.7332e+01 -1.7402e+01  5e-03  6e-05  4e-03  3e-03\n",
        " 3: -1.7333e+01 -1.7334e+01  5e-05  6e-07  4e-05  3e-05\n",
        " 4: -1.7333e+01 -1.7333e+01  5e-07  6e-09  4e-07  3e-07\n",
        " 5: -1.7333e+01 -1.7333e+01  5e-09  6e-11  4e-09  3e-09\n",
        "Optimal solution found.\n",
        "[ 1.61e+00]\n",
        "[ 2.66e+00]\n",
        "[ 1.29e+00]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We actually have a bit of a problem $-$ we cannot take a fractional number of classes, that would be quite messy. So we must impose integrality constraints. There are a number of options for mixed integer linear programming (MILP), but we can again use `cvxopt` via the solver `glpk` in `cvxopt.glpk.ilp.`"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Advanced Project 3B: The Best Christmas</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the basics are starting to get easy for you, try out this more advanced analysis $-$ perhaps a little off-season, but Santa does work year-round. In this optional advanced module project, we will look at the <a href=\"https://www.kaggle.com/c/helping-santas-helpers/\">Santa's Helpers</a> Kaggle competition from just earlier this year. While we won't be able to provide guided assistance to everyone who chooses to try out the optional project, do have fun with it and if you think you've made it pretty far, consider applying to be a project director rather than an analyst following the comp! "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<center><font face='Garamond'>\u00a9 Data Ventures Global</font></center>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}