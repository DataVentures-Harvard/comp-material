{
 "metadata": {
  "name": "",
  "signature": "sha256:e14796adbe82e33587eeafffaded2da5aedd7bdba33c1c4468675f3b0e2001ab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"#900000\">Econometrics I: Linear and Generalized Linear Models</font></font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"#5C3317\">(i.e. Regressions, A \"<i>Straight</i>forward\" Supervised Learning Model)</font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Ventures, Harvard College Division | Spring 2015 | Session 1A: Supervised Learning: Univariate and Multivariate Linear Regressions, Binary and Multiclass Classification with Logistic Regressions, Overfitting and Regularization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<center>\n",
      "<img style=\"width:250px\" src=\"http://www.rickstewart.com/sites/default/files/regression-analysis1.jpg\" alt=\"Regressions Cartoon.\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"000099\">Introduction</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A small part of the large domain of supervised machine learning, regressions are possibly the most commonly used method for crunching data, and are a foundational tool in econometrics. They constitute relatively simple supervised learning models and their use is motivated by the Central Limit Theorem on the error term (no worries if this does not quite make sense, yet). Note that these assumptions of Normality will not always be valid, and so regression analyses will not be an invariably legitimate approach, though it is often a good first place to start. We go over univariate and multivariate linear regressions, including nonlinear polynomial regressions, binary and multiclass classification with logistic regression, and how to address the problem of overfitting in these scenarios with regualarization. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"000099\">Linear Models: Univariate Linear Regressions</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In univariate linear regression, where the $x_i$'s are our input variables and the $y_i$'s are our output variables, our hypothesis function that we infer from the training set and use to predict over the test set is of the form $\\hat y (x) = \\theta_0 + \\theta_1 x$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img style=\"width:300px\" src=\"http://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg\" alt=\"Linear Regression. Courtesy of Wikipedia.\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to fit the best possible straight line and to do do we must find parameters $\\theta_0$ and $\\theta_1$ such that we minimize the difference between the prediction $ \\hat y $ and the actual $y$ in the training set. We essentially are faced with an optimization problem (optimization will be covered as a separately unit in the weeks to come), in which we seek to minimize the squared errors in the following objective function, or cost function $C$ over $n$ training examples. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>$C(\\theta_0, \\theta_1)$ = $\\min_{\\theta_0, \\theta_1}$ $\\frac{1}{2n} \\sum_{i=1}^{n} (\\hat y (x^{(i)}) - y^{(i)})^2$</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can visualize this as descending the contours to find the minimum of a 3D surface plot (of course, in the case of a single parameter $\\theta_1$ this trivializes to finding the minimum of a parabolic function): \n",
      "\n",
      "<center>\n",
      "<img style=\"width:400px\" src=\"http://www.astro.umd.edu/~cychen/MATLAB/ContourPlots_05.png\" alt=\"Matlab Plot. Courtesy of the University of Maryland.\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above optimization problem can be solved numerically by gradient descent or analytically by a bit of statistics. When we refer to being able to solve something numerically, we refer to iterative approaches in arriving closer to the solution. While we discuss gradient descent for geometric intuition on the nature of the problem, for most practical purposes you need not implement these yourself. In contrast, when we refer to being able to solve something analytically, we refer to direct and non-iterative solutions, and so you should be able to directly implement these yourself in Python. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Geometric Intuition: Gradient Descent**. In this algorithm, we iteratively slope down the cost, i.e. least squares, function until we converge to the global minimum (which happens to be the only local minimum in our ordinary least squares regression) by computing $\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} C(\\theta_0, \\theta_1)$ for $j=0$ and $j=1$ in each of the iterations until convergence (which occurs when the new parameters values computed no longer change significantly or at all from the previous computed values). $\\frac{\\partial}{\\partial \\theta_j} C(\\theta_0, \\theta_1)$ represents the partial derivative of the least squares function with respect to the $j^{th}$ parameter. The learning rate $\\alpha$ controls how large of a descent we take in each iteration of the algorithm. If $\\alpha$ is too small the algorithm may take a long time to converge and if it is too large the algorithm may never converge since it is possible to keep overshooting our desired minimum each time. If we have already reached the minimum, the value of $\\alpha$ will not matter. Given that we have started off with an appropriate learning rate, there will be no need to adjust $\\alpha$ as we approach the minimum since the slope will be less steep and thus we take a smaller descent along each successive iteration. Gradient descent scales well for larger datasets, but we have to take into account additional factors affecting rates of convergence including the different orders of magnitude for different predictors in the multivariate case. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**A Direct Analytical Solution.** For the univariate model (and for the multivariate case as well) we will be able to directly determine the \"least squares\" fit, where for the slope $\\theta_1$ and the intercept $\\theta_0$ we have:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>$\\theta_1 \\; = \\; \\bar xy - \\bar x \\bar y   \\; \\; / \\; \\;  \\bar {x^2} - \\bar x^2$</center>\n",
      "\n",
      "<center>$\\theta_0$ = $\\bar y - \\theta_1 \\bar x$</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us code this up together in function called `univariateLinearRegression`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a function that return the slope and intercept of the univariate linear regression on input predictor and output lists \n",
      "def univariateLinearRegression(x, y):\n",
      "    sum_x2 = sum([elem**2 for elem in x])\n",
      "    sum_xy = sum([x[i]*y[i] for i in range(len(x))])\n",
      "    theta1 = (sum_xy-(sum(x)*sum(y))/len(x)) / (sum_x2-((sum(x)**2)/len(x)))\n",
      "    theta0 = (sum(y)-sum(x)*theta1)/len(x)\n",
      "    return theta1, theta0 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Can you see how the equations are implemented in the function (recall the Pythonic notation from `Session 0C`)? Can you also see that the above implementation produces \"concise\" yet \"inefficient\" code? Note that as we scale up to larger datasetss, runtime efficiency can become fairly important to consider. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our above function is quite nice, but such commonly used methods are probably already written in Python! For example, the library `numpy` has a method called `polyfit` where we set the degree of our polynomial to be 1 (for a univariate linear regression). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np \n",
      "def univariateLinReg(x, y): return np.polyfit(x, y, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that `univariateLinReg` and `univariateLinearRegression` return exactly the same thing! If you remember the debugging tips from `Session 0C`, do check out the documentation on polyfit here: <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html\">numpy.polyfit</a>. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"000099\">Still \"Linear\" Models: Polynomial Regressions</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In reality, most models are nonlinear, but we can still use linear models to fit nonlinear, polynomial data by considering in addition to the predictor $x$, the predictor $x^2$ (this is linearly independent of $x$) so that we have $\\hat y = \\theta_0 + ... + \\theta_d x^d$ where $d$ is the highest degree of the polynomial. Note that our friend <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html\">numpy.polyfit</a> can also be used again here. With more than a single predictor variable, we can combine terms to produce fairly sophisticated fits, e.g. $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_{12} x_1 x_2 + \\theta_{22} x^2 + ... $. As in the univariate case, the multivariate case also has a direct analytical solution to the best \"least squares\" fit (see next section). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cubicFit(x, y): return np.polyfit(x, y, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"000099\">More Linear Models: Multivariate Linear Regressions</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Things get more interesting and far more useful when dealing with multiple features or attributes. While let visually intuitive, multivariate linear regression becomes an instant powerhouse for matrix algebra. Our predictive model now becomes $\\hat y (x) = \\theta_0 + \\theta_1 x_1 + ... + \\theta_k x_k$ for $k$ features, or attributes. \n",
      "\n",
      "<center>\n",
      "<img style=\"width:400px\" src=\"http://www.mathworks.com/help/examples/stats/EstimateMultipleLinearRegressionCoefficientsExample_01.png\" alt=\"Two Attributes/Features in Linear Regression. Courtesy of Mathworks.\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Normal Equations Method**. This provides a direct analytical \"least squares\" solution to the parameters of a multivariate linear regression model:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>$\\theta$ = $(X^T X)^{-1} X^T y$</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $\\theta$ is the column vector of model parameters to determine with $\\theta_0$ (the intercept) and $\\theta_i$ for $i \\geq 1$ (the \"slope\" coefficients), $X$ is the matrix of predictor variables (also called features, attributes), where the $(i+1)^{th}$ column corresponds to the $i^{th}$ predictor variable (column $0$ represents the coefficients of the \"intercept\" term, and is a column of $1$'s), and $y$ is the column vector. (Adapting the notation from `Session OB`, the \"T\" represents the transpose and the \"-1\" represents the inverse.) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Try implementing the above equation in Python and compare the output with Python's own libraries for multivariate linear regression. For the latter we make use of library `sklearn` (will be very useful in the projects to come) below: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "# multivariate linear regression where X is a list of lists, each inner list corresponding to a y-output\n",
      "def multiLinReg(X, y): \n",
      "    regr = linear_model.LinearRegression()\n",
      "    regr.fit(X, y)\n",
      "    coefficients = [regr.intercept_]\n",
      "    coefficients.extend(regr.coef_)\n",
      "    return coefficients"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"000099\">Generalized Linear Models: Logistic Regression and Binary Classification</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above are a part of a broader class of \"Generalized Linear Models\", which include the classification models defined by logistic regression. In binary classification, we seek to predice between two classes (0 or 1, can be thought of as the negative and positive classes). The predictive form for this is of the form "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center> $p_1 = (1 \\;  +   \\; exp(-\\theta^T x))^{-1} = P(\\; y = 1 \\; | \\; x, \\theta\\; )$ </center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where the prediction $p_1$ here is the probability of being in the positive class (i.e. being $1$). Note that classes can be define arbitrarily. For example, let us say we have predictors $x$ that reflected total commute time during the day for a Harvard sophomore, and our probability $p_1$ could define the probability that the student resides in the Quad or the probability that the student resides near the River, depending on whether Categories $0$ and $1$ are the Quad and River (or the River and Quad, respectively). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Actually assigning a class of 0 or 1 based on the predictor variables can be defined by a simple rule: if $p_1 \\geq 1/2$ then we predict 1, else we predict 0. A probability greater than what we would find at random (1 over the number of classes, in general) occurs if $\\theta^T X \\geq 0$. Can you see why? - The math behind this can be derived quite simply. This concept allows us to construct a decision boundary, such as the below with two predictor variables $x_1$ and $x_2$. The decision boundary defined in black does a fairly good job of separating these binary classes. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img style=\"width:450px\" src=\"http://www.onthelambda.com/wp-content/uploads/2014/07/quadratic.png\" alt=\"Two Attributes/Features in Linear Regression. Courtesy of Mathworks.\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"000099\">Generalized Linear Models: Logistic Regression and Multiclass Classification</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With multiclass classification, i.e. in the case of more than two categorical problems, the classification problem can still be reduced to the binary case if we apply logistic regression in a \"one v all\" or \"one v rest\" algorithm. Let us consider the House possibilities of a sophomore. We have thirteen total classes, which includes the twelves classes and the possibility of being an off-campus student. With the algorithm, we select the target category of interest to be \"1\" and aggregate the remaining classes as \"0.\" Let us say we were interested in whether a student resided in Currier House - Currier would be the class \"1\" and the remaining non-Currier Houses would together constitute the negative class \"0.\" The probabilities that we generate this way for each of the classes can be used to construct a multinomial model, where for $n$ students, the probability of finding $n_a$ students in House A, $n_b$ students in House B, and so on is given by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center> $P(Houses = houses) = n! (n_a! n_b! ... n_m!)^{-1} p_{a}^{n_a} p_{b}^{n_b} ... p_{m}^{n_m} $ </center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us actually try this out again using `sklearn.` Notice the bit of similarity of this function to `multiLinReg` in determining the coefficients. Notice that in order to obtain an intercept or bias term in $\\theta^T X$ we must specify obtaining an intercept term with the argument \"fit_intercept=True\" within the \"LogisticRegression()\" method. What we end up returning is a matrix of probabilties where the columns correspond to different classes and each row (list within the metalist) must sum to 1, where the row represents the different class probabilities for a specific item whose predictor variables $x$ are known. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logReg(X, y): \n",
      "    logr = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6) \n",
      "    logr.fit(X, y)\n",
      "    return logr.predict_proba(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What else might be we be interested in computing? With the probabilities given by the above function we could immediately make class assignments. Try this out yourself and compare your implementation with the output `logr.predict(X).` There are further useful functions beyond these key basics and are documented extensively in <a href=\" http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">sklearn logistic regression</a>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"000099\">Thinking About More Advanced Methods: Regularization to Reduce Overfitting</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finding the \"best\" model is a tradeoff between fitting the data well and still being able to generalize well to further data in the future, i.e. respectively avoiding underfitting and avoiding overfitting. Regularization is a tool that maybe useful in certain cases where to avoid overfitting, we essentially penalize higher order terms by multiplying them by large coefficients so that the \"least squares\" fit for the parameters $\\theta$ yields smaller $\\theta$-values for these predictors that we've penalized. We will visit regularization again in future applications. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"black\">Questions?</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us help you! Please direct all questions on material covered to your session leader before and during the module assignment. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"900000\">Mini-Project 1B: Housing Prices</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While the above can be applied to of course a wide variety of applications, what's econometrics without an econ application? In `Session 1B` we will be working with a housing prices dataset among the toy datasets provided by `sklearn.` Learn more about the datasets relevant for regressions here: http://scikit-learn.org/stable/datasets/. \n",
      "\n",
      "Please find a group of three students for this brief assignment, due at the end of the next comp session, where we will also be illustrating the above theory with this and other toy datasets, time permitting. Your selected group will be the same group of friends you will work with throughout the segments on econometrics and should ideally be a group of students different from those who you choose to work with for the segments on optimization/operations research and machine learning/bayesian statistics. Please submit one notebook per group (do not send the `.ipynb` but rather the `nbviewer` linked page to your module leader with all your group members listed in the body of the email). Assignments will be mostly be evaluated on computational accuracy. We will also provide feedback on your logic and request all code to be aptly commented. We will go over the project together at the next session. If you are confused about anything regarding this project or the concepts discussed, please reach out to us via email. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even for these group module assignments, we do not expect the work to be very length so just put in your best foot forward! You may choose to write in a language other than Python, and if so you must comment out the code you submit below in addition to submitting separate .r or .m files (if using R or Matlab). Unfortunately, we do not provide extensive support beyond Python, but we will provide support for other languages in the future. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"900000\">Advanced Project 1B: Molecular Responses</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The past Kaggle competition we will look at is <a href=\"http://www.kaggle.com/c/bioresponse\">Biological Responses</a> from 2012. What sort of general data problem are we trying to solve here? What methods would be relevant? What correponding libraries in Python would be useful? Is the solution simple enough to be able to implement the math yourself? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Get the data.** Please load the csv files from: http://www.kaggle.com/c/bioresponse/data. Have fun and don't be afraid of making mistakes along the way!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<center><font face='Garamond'>\u00a9 Data Ventures Global</font></center>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}